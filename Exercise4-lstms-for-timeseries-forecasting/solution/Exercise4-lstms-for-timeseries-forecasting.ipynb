{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: LSTMs for time-series forecasting\n",
        "\n",
        "In the field of time-series forecasting, one popular technique is the Long Short-Term Memory (LSTM) model. LSTM is a type of recurrent neural network (RNN) that is specifically designed to handle sequential data, making it well-suited for analyzing time-series data. In this example, we will explore how an LSTM can be applied to forecast weather patterns using historical data from the National Oceanic and Atmospheric Administration (NOAA).\n",
        "\n",
        "*  Data collection and inspection\n",
        "\n",
        "Gather historical weather data from the National Oceanic and Atmospheric Administration (NOAA) or any other reliable source. This data should include variables such as temperature, humidity, wind speed, and precipitation, along with corresponding timestamps.\n",
        "\n",
        "* Data preprocessing\n",
        "\n",
        "Clean and preprocess the collected data to ensure it is in a suitable format for training the LSTM model. This may involve steps such as handling missing values, normalizing the data, and splitting it into training and testing sets.\n",
        "\n",
        "* Model training\n",
        "\n",
        "Build and train the LSTM model using the preprocessed data. This involves defining the architecture of the LSTM network, specifying the number of layers and neurons, and selecting appropriate activation functions and optimization algorithms. Train the model using the training set and adjust the model parameters to minimize the prediction error.\n",
        "\n",
        "* Model evaluation\n",
        "\n",
        "Evaluate the performance of the trained LSTM model using the testing set. Calculate metrics such as mean squared error (MSE) or root mean squared error (RMSE) to assess the accuracy of the model's predictions. Compare the predicted weather patterns with the actual observed values to determine the model's effectiveness.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"3477997.csv\")\n",
        "df.columns\n",
        "df[[\"STATION\", \"TAVG\"]].groupby(\"STATION\").count().sort_values(\"TAVG\", ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[df.STATION == \"USW00023254\"][[\"TAVG\"]].plot()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "! pip install -q keras\n",
        "! wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        " \n",
        "df = pd.read_csv('airline-passengers.csv')\n",
        "timeseries = df[[\"Passengers\"]].values.astype('float32')\n",
        " \n",
        "# train-test split for time series\n",
        "train_size = int(len(timeseries) * 0.67)\n",
        "test_size = len(timeseries) - train_size\n",
        "train, test = timeseries[:train_size], timeseries[train_size:]\n",
        " \n",
        "def create_dataset(dataset, lookback):\n",
        "    \"\"\"Transform a time series into a prediction dataset\n",
        "    \n",
        "    Args:\n",
        "        dataset: A numpy array of time series, first dimension is the time steps\n",
        "        lookback: Size of window for prediction\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(dataset)-lookback):\n",
        "        feature = dataset[i:i+lookback]\n",
        "        target = dataset[i+1:i+lookback+1]\n",
        "        X.append(feature)\n",
        "        y.append(target)\n",
        "    return torch.tensor(X), torch.tensor(y)\n",
        " \n",
        "lookback = 4\n",
        "X_train, y_train = create_dataset(train, lookback=lookback)\n",
        "X_test, y_test = create_dataset(test, lookback=lookback)\n",
        " \n",
        "class AirModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=1, batch_first=True)\n",
        "        self.linear = nn.Linear(50, 1)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        " \n",
        "model = AirModel()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.MSELoss()\n",
        "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n",
        " \n",
        "n_epochs = 2000\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader:\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation\n",
        "    if epoch % 100 != 0:\n",
        "        continue\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_train)\n",
        "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
        "        y_pred = model(X_test)\n",
        "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
        "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n",
        " \n",
        "with torch.no_grad():\n",
        "    # shift train predictions for plotting\n",
        "    train_plot = np.ones_like(timeseries) * np.nan\n",
        "    y_pred = model(X_train)\n",
        "    y_pred = y_pred[:, -1, :]\n",
        "    train_plot[lookback:train_size] = model(X_train)[:, -1, :]\n",
        "    # shift test predictions for plotting\n",
        "    test_plot = np.ones_like(timeseries) * np.nan\n",
        "    test_plot[train_size+lookback:len(timeseries)] = model(X_test)[:, -1, :]\n",
        "# plot\n",
        "plt.plot(timeseries)\n",
        "plt.plot(train_plot, c='r')\n",
        "plt.plot(test_plot, c='g')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download the training data.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', \n",
        "                               train=True, \n",
        "                               download=True, \n",
        "                               transform=transforms.ToTensor())\n",
        "\n",
        "validation_dataset = datasets.MNIST('./data', \n",
        "                                    train=False, \n",
        "                                    transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                                batch_size=batch_size, \n",
        "                                                shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We iterate over the training loader to see that we get the following:\n",
        "# - X_train: a tensor of size (batch_size, 1, 28, 28) with a batch of images\n",
        "# - y_train: a tensor of size (batch_size) with the corresponding labels\n",
        "\n",
        "for (X_train, y_train) in train_loader:\n",
        "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
        "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the first 10 training digits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pltsize=1\n",
        "plt.figure(figsize=(10*pltsize, pltsize))\n",
        "\n",
        "for i in range(10):\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray_r\")\n",
        "    plt.title(f'Class: {y_train[i].item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multilayer perceptron (MLP) network\n",
        "\n",
        "Let's define the network as a Python class.  We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the model as an MLP with two hidden layers of size 50 and ReLU activations.\n",
        "# Read more about the different types of layers here: http://pytorch.org/docs/nn.html\n",
        "# We also add dropout layers to prevent overfitting.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, 50) # 28*28 is the size of the input image\n",
        "        self.fc1_drop = nn.Dropout(0.2) # Dropout layers help prevent overfitting\n",
        "\n",
        "        self.fc2 = nn.Linear(50, 50) # 50 is the size of the hidden layer\n",
        "        self.fc2_drop = nn.Dropout(0.2) # Dropout layers help prevent overfitting\n",
        "\n",
        "        self.fc3 = nn.Linear(50, 10) # 10 is the size of the output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28) # Flatten the input image\n",
        "\n",
        "        x = F.relu(self.fc1(x)) # Apply ReLU activation to the first layer\n",
        "        x = self.fc1_drop(x) # Apply dropout to the first layer\n",
        "\n",
        "        x = F.relu(self.fc2(x)) # Apply ReLU activation to the second layer\n",
        "        x = self.fc2_drop(x) # Apply dropout to the second layer\n",
        "\n",
        "        # Apply log softmax to the output layer to get a probability distribution\n",
        "        # over the 10 classes of digits\n",
        "        return F.log_softmax(self.fc3(x), dim=1) \n",
        "\n",
        "# We instantiate the model and move it to the GPU if available.\n",
        "model = Net().to(device)\n",
        "\n",
        "# We use Stachastic Gradient Descent (SGD) as our optimizer and Cross Entropy as our loss function.\n",
        "# See http://pytorch.org/docs/optim.html#algorithms for more information about the different optimization\n",
        "# algorithms PyTorch offers.\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning\n",
        "\n",
        "Let's now define functions to `train()` and `validate()` the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def train(training_loss_values, epoch, log_interval=200):\n",
        "    model.train() # Set model to training mode\n",
        "    val_loss = 0\n",
        "    # Loop over each batch from the training set\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device) # Copy data to GPU if needed\n",
        "        target = target.to(device) # Copy target to GPU if needed\n",
        "\n",
        "        # Run an optimization step. See https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step\n",
        "        optimizer.zero_grad() # Zero out gradients from previous step\n",
        "        output = model(data) # Pass data through the network\n",
        "        loss = loss_fn(output, target) # Calculate loss\n",
        "        val_loss = loss.data.item() # Calculate loss\n",
        "        loss.backward() # Backpropagation\n",
        "        optimizer.step() # Update weights\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
        "    val_loss /= len(train_loader) # Calculate average loss\n",
        "    training_loss_values.append(val_loss) # Add to loss vector\n",
        "\n",
        "def validate(loss_vector, accuracy_vector):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    val_loss, correct = 0, 0 # Initialize loss and number of correct classifications to 0\n",
        "    for data, target in validation_loader:\n",
        "        \n",
        "        data = data.to(device) # Copy data to GPU if needed\n",
        "        target = target.to(device) # Copy target to GPU if needed\n",
        "        output = model(data) # Pass data through the network\n",
        "        val_loss += loss_fn(output, target).data.item() # Calculate loss\n",
        "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data).cpu().sum() # Add number of correct classifications\n",
        "\n",
        "    val_loss /= len(validation_loader) # Calculate average loss\n",
        "    loss_vector.append(val_loss) # Add to loss vector\n",
        "\n",
        "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset) # Calculate accuracy\n",
        "    accuracy_vector.append(accuracy) # Add to accuracy vector\n",
        "    \n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        val_loss, correct, len(validation_loader.dataset), accuracy))\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now train our model for 10 epochs and afterwards plot the training and validation losses over the training epochs. An epoch is a single pass through the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "epochs = 10\n",
        "\n",
        "training_loss_values, loss_values, accuracy_values = [], [], []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(training_loss_values, epoch)\n",
        "    validate(loss_values, accuracy_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now plot the training and validation losses over the training epochs.\n",
        "\n",
        "Loss is a measure of how far a model's predictions are from its label, i.e. how bad the model is. The goal of training a model is to find a set of parameters that minimizes the loss. The loss is calculated using a loss function, which takes the model's prediction and the correct label as input and returns a number. The lower the loss, the better the model.\n",
        "\n",
        "The training process tries to find the set of parameters that minimizes the loss on the training data, i.e. the training loss. However, we also want to know how well the model performs on data it has never seen before, i.e. the validation data. If the model performs much worse on the validation data than the training data, it is memorizing patterns in the training data rather than learning general rules that apply to data in general. This is called overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(np.arange(1, epochs + 1), training_loss_values)\n",
        "plt.title('training loss')\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(np.arange(1, epochs + 1), loss_values)\n",
        "plt.title('validation loss')\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(np.arange(1, epochs + 1), accuracy_values)\n",
        "plt.title('validation accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Show model predictions and the true labels for a few images\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "data, label = next(iter(validation_loader)) # Get a batch of validation data (batch size = 32)\n",
        "data = data.to(device) # Send data to device\n",
        "output = model(data) # Pass data through the network\n",
        "pred = output.data.max(1)[1] # get the index of the max log-probability for each element in the batch\n",
        "\n",
        "# Create a 5x5 grid and show the predicted and true labels for each of 25 images\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "for idx in np.arange(25):\n",
        "    ax = fig.add_subplot(5, 5, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(data[idx].cpu().numpy().reshape(28,28), cmap='gray_r')\n",
        "    ax.set_title(\"pred={} (truth={})\".format(str(pred[idx].item()), str(label[idx].item())), \n",
        "                 color=(\"green\" if pred[idx]==label[idx] else \"red\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model tuning\n",
        "\n",
        "Modify the MLP model. Try to improve the classification accuracy, or experiment with the effects of different parameters.\n",
        "\n",
        "First try changing the learning rate `lr` for the optimizer and run the notebook again.\n",
        "\n",
        "Did you increase or decrease the learning rate? What was the effect on the training and validation losses? Try the opposite and see what happens.\n",
        "\n",
        "Next, try adding an additional hidden layer to the MLP model. How does this affect the training and validation losses?\n",
        "\n",
        "Finally, try changing the number of hidden units a the hidden layer. How does this affect the training and validation losses?\n",
        "\n",
        "You can also consult the PyTorch documentation at http://pytorch.org/."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
