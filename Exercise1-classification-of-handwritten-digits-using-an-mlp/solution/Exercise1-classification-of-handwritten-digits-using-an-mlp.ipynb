{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification of handwritten digits using an MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies for this notebook\n",
        "! pip install -q torch matplotlib torchvision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download the training data.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    \"./data\", train=True, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "validation_dataset = datasets.MNIST(\n",
        "    \"./data\", train=False, transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    dataset=validation_dataset, batch_size=batch_size, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We iterate over the training loader to see that we get the following:\n",
        "# - X_train: a tensor of size (batch_size, 1, 28, 28) with a batch of images\n",
        "# - y_train: a tensor of size (batch_size) with the corresponding labels\n",
        "\n",
        "for X_train, y_train in train_loader:\n",
        "    print(\"X_train:\", X_train.size(), \"type:\", X_train.type())\n",
        "    print(\"y_train:\", y_train.size(), \"type:\", y_train.type())\n",
        "\n",
        "    # we just want to show the dimensions of the first batch\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the first 10 training digits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pltsize = 1\n",
        "plt.figure(figsize=(10 * pltsize, pltsize))\n",
        "\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i + 1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap=\"gray_r\")\n",
        "    plt.title(f\"Class: {y_train[i].item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multilayer perceptron (MLP) network\n",
        "\n",
        "Let's define the network as a Python class.  We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the model as an MLP with two hidden layers of size 50 and ReLU activations.\n",
        "# Read more about the different types of layers here: http://pytorch.org/docs/nn.html\n",
        "# We also add dropout layers to prevent overfitting.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(28 * 28, 50)  # 28*28 is the size of the input image\n",
        "        self.fc1_drop = nn.Dropout(0.2)  # Dropout layers help prevent overfitting\n",
        "\n",
        "        self.fc2 = nn.Linear(50, 50)  # 50 is the size of the hidden layer\n",
        "        self.fc2_drop = nn.Dropout(0.2)  # Dropout layers help prevent overfitting\n",
        "\n",
        "        self.fc3 = nn.Linear(50, 10)  # 10 is the size of the output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input image\n",
        "\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first layer\n",
        "        x = self.fc1_drop(x)  # Apply dropout to the first layer\n",
        "\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation to the second layer\n",
        "        x = self.fc2_drop(x)  # Apply dropout to the second layer\n",
        "\n",
        "        # Apply log softmax to the output layer to get a probability distribution\n",
        "        # over the 10 classes of digits\n",
        "        return F.log_softmax(self.fc3(x), dim=1)\n",
        "\n",
        "\n",
        "# We instantiate the model and move it to the GPU if available.\n",
        "model = Net().to(device)\n",
        "\n",
        "# We use Stachastic Gradient Descent (SGD) as our optimizer and Cross Entropy as our loss function.\n",
        "# See http://pytorch.org/docs/optim.html#algorithms for more information about the different optimization\n",
        "# algorithms PyTorch offers.\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning\n",
        "\n",
        "Let's now define functions to `train()` and `validate()` the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(training_loss_values, epoch, log_interval=200):\n",
        "    model.train()  # Set model to training mode\n",
        "    val_loss = 0\n",
        "    # Loop over each batch from the training set\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)  # Copy data to GPU if needed\n",
        "        target = target.to(device)  # Copy target to GPU if needed\n",
        "\n",
        "        # Run an optimization step. See https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step\n",
        "        optimizer.zero_grad()  # Zero out gradients from previous step\n",
        "        output = model(data)  # Pass data through the network\n",
        "        loss = loss_fn(output, target)  # Calculate loss\n",
        "        val_loss = loss.data.item()  # Calculate loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.data.item(),\n",
        "                )\n",
        "            )\n",
        "    val_loss /= len(train_loader)  # Calculate average loss\n",
        "    training_loss_values.append(val_loss)  # Add to loss vector\n",
        "\n",
        "\n",
        "def validate(loss_vector, accuracy_vector):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss, correct = (\n",
        "        0,\n",
        "        0,\n",
        "    )  # Initialize loss and number of correct classifications to 0\n",
        "    for data, target in validation_loader:\n",
        "        data = data.to(device)  # Copy data to GPU if needed\n",
        "        target = target.to(device)  # Copy target to GPU if needed\n",
        "        output = model(data)  # Pass data through the network\n",
        "        val_loss += loss_fn(output, target).data.item()  # Calculate loss\n",
        "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
        "        correct += (\n",
        "            pred.eq(target.data).cpu().sum()\n",
        "        )  # Add number of correct classifications\n",
        "\n",
        "    val_loss /= len(validation_loader)  # Calculate average loss\n",
        "    loss_vector.append(val_loss)  # Add to loss vector\n",
        "\n",
        "    accuracy = (\n",
        "        100.0 * correct.to(torch.float32) / len(validation_loader.dataset)\n",
        "    )  # Calculate accuracy\n",
        "    accuracy_vector.append(accuracy)  # Add to accuracy vector\n",
        "\n",
        "    print(\n",
        "        \"\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            val_loss, correct, len(validation_loader.dataset), accuracy\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now train our model for 10 epochs and afterwards plot the training and validation losses over the training epochs. An epoch is a single pass through the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "epochs = 10\n",
        "\n",
        "training_loss_values, loss_values, accuracy_values = [], [], []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(training_loss_values, epoch)\n",
        "    validate(loss_values, accuracy_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now plot the training and validation losses over the training epochs.\n",
        "\n",
        "Loss is a measure of how far a model's predictions are from its label, i.e. how bad the model is. The goal of training a model is to find a set of parameters that minimizes the loss. The loss is calculated using a loss function, which takes the model's prediction and the correct label as input and returns a number. The lower the loss, the better the model.\n",
        "\n",
        "The training process tries to find the set of parameters that minimizes the loss on the training data, i.e. the training loss. However, we also want to know how well the model performs on data it has never seen before, i.e. the validation data. If the model performs much worse on the validation data than the training data, it is memorizing patterns in the training data rather than learning general rules that apply to data in general. This is called overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(np.arange(1, epochs + 1), training_loss_values)\n",
        "plt.title(\"training loss\")\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(np.arange(1, epochs + 1), loss_values)\n",
        "plt.title(\"validation loss\")\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(np.arange(1, epochs + 1), accuracy_values)\n",
        "plt.title(\"validation accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show model predictions and the true labels for a few images\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "data, label = next(\n",
        "    iter(validation_loader)\n",
        ")  # Get a batch of validation data (batch size = 32)\n",
        "data = data.to(device)  # Send data to device\n",
        "output = model(data)  # Pass data through the network\n",
        "pred = output.data.max(1)[\n",
        "    1\n",
        "]  # get the index of the max log-probability for each element in the batch\n",
        "\n",
        "# Create a 5x5 grid and show the predicted and true labels for each of 25 images\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "for idx in np.arange(25):\n",
        "    ax = fig.add_subplot(5, 5, idx + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(data[idx].cpu().numpy().reshape(28, 28), cmap=\"gray_r\")\n",
        "    ax.set_title(\n",
        "        \"pred={} (truth={})\".format(str(pred[idx].item()), str(label[idx].item())),\n",
        "        color=(\"green\" if pred[idx] == label[idx] else \"red\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model tuning\n",
        "\n",
        "Modify the MLP model. Try to improve the classification accuracy, or experiment with the effects of different parameters.\n",
        "\n",
        "First try changing the learning rate `lr` for the optimizer and run the notebook again.\n",
        "\n",
        "Did you increase or decrease the learning rate? What was the effect on the training and validation losses? Try the opposite and see what happens.\n",
        "\n",
        "Next, try adding an additional hidden layer to the MLP model. How does this affect the training and validation losses?\n",
        "\n",
        "Finally, try changing the number of hidden units a the hidden layer. How does this affect the training and validation losses?\n",
        "\n",
        "You can also consult the PyTorch documentation at http://pytorch.org/."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
