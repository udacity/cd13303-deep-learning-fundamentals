{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 1: Classification of handwritten digits using an MLP\n",
        "\n",
        "In this exercise, you will train a multi-layer perceptron (MLP) to classify handwritten digits from the MNIST dataset. The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0 to 9). The task is to classify each image into one of the 10 classes (one for each digit).\n",
        "\n",
        "The steps you will follow are:\n",
        "* Set up the environment\n",
        "* Load the dataset\n",
        "* Define the model\n",
        "* Train the model\n",
        "* Evaluate the model"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the environment"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the environment for this notebook\n",
        "\n",
        "import torch\n",
        "\n",
        "# Set up the device for model training\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset\n",
        "\n",
        "We obtain the MNIST data using the torchvision.datasets module. The first time we run this code, the data will be downloaded and stored in a local directory (`./data`). After that, the data will be directly loaded from the local directory."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the MNIST dataset\n",
        "dataset = load_dataset(\"mnist\")\n",
        "\n",
        "\n",
        "def transform_func(examples):\n",
        "    return {\n",
        "        \"image\": [np.array(example).astype(float).reshape(1, 28, 28) / 255 for example in examples[\"image\"]],\n",
        "        \"label\": examples[\"label\"],\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "dataset = dataset.with_transform(transform_func)\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    dataset[\"train\"], batch_size=32, shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(2, 5)\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(dataset[\"test\"][i][\"image\"].squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(dataset[\"test\"][i][\"label\"])\n",
        "    ax.set_axis_off()\n",
        "\n",
        "\n",
        "for batch in dataloader_train:\n",
        "    print(type(batch))\n",
        "    # print(batch[\"image\"].shape)\n",
        "    # print(batch[\"label\"].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from torchvision import datasets, transforms\n",
        "\n",
        "# train_dataset = datasets.MNIST(\n",
        "#     \"./data\", train=True, download=True, transform=transforms.ToTensor()\n",
        "# )\n",
        "\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     dataset=train_dataset, batch_size=128, shuffle=True\n",
        "# )\n",
        "\n",
        "# # Show images from the training dataset\n",
        "# fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
        "# for i in range(10):\n",
        "#     # Show the labels\n",
        "#     ax[i].text(2.5, -2, str(i), fontsize=18)\n",
        "#     ax[i].imshow(train_dataset[i][0].squeeze(), cmap=\"gray\")\n",
        "#     ax[i].axis(\"off\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model\n",
        "\n",
        "Here we define our Multi-Layer Perceptron (MLP) network.  Recall that the MLP is composed of a series of fully-connected layers.  In this case, we will define a network with two hidden layers.\n",
        "\n",
        "In PyTorch, we define our MLP using a class. We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass.\n",
        "\n",
        "Read through this code line-by-line, and follow the comments explaining what each line does. If you don't get everything, that's ok. If there is a new term you do not understand, e.g. ReLU, please feel free to look it up on the internet.\n",
        "\n",
        "Also, we will be coving PyTorch more in-depth in future exercises."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Define the model as an MLP with two hidden layers of size 50 and ReLU activations.\n",
        "# Read more about the different types of layers here: http://pytorch.org/docs/nn.html\n",
        "# We also add dropout layers to prevent overfitting.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(28 * 28, 50)  # 28*28 is the size of the input image\n",
        "        self.fc1_drop = nn.Dropout(0.2)  # Dropout layers help prevent overfitting\n",
        "\n",
        "        self.fc2 = nn.Linear(50, 50)  # 50 is the size of the hidden layer\n",
        "        self.fc2_drop = nn.Dropout(0.2)  # Dropout layers help prevent overfitting\n",
        "\n",
        "        self.fc3 = nn.Linear(50, 10)  # 10 is the size of the output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input image\n",
        "\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first layer\n",
        "        x = self.fc1_drop(x)  # Apply dropout to the first layer\n",
        "\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation to the second layer\n",
        "        x = self.fc2_drop(x)  # Apply dropout to the second layer\n",
        "\n",
        "        # Apply log softmax to the output layer to get a probability distribution\n",
        "        # over the 10 classes of digits\n",
        "        return F.log_softmax(self.fc3(x), dim=1)\n",
        "\n",
        "\n",
        "# We instantiate the model and move it to the GPU if available.\n",
        "model = Net().to(device)\n",
        "\n",
        "# We use Stachastic Gradient Descent (SGD) as our optimizer and Cross Entropy as our loss function.\n",
        "# See http://pytorch.org/docs/optim.html#algorithms for more information about the different optimization\n",
        "# algorithms PyTorch offers.\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "While training the model, we will monitor its performance on the validation set. Sometimes we use the performance on the validation set to make decisions about when to stop training the model (e.g. if the performance on the validation set starts to degrade, we may stop training early to avoid overfitting).\n",
        "\n",
        "Let's now define functions to `train()` and `validate()` the model. "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions to train and validate the model\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss = 0\n",
        "    # Loop over each batch from the training set\n",
        "    for batch_idx, (data, target) in enumerate(dataloader_train):\n",
        "        print(data, target)\n",
        "        data = data.to(device)  # Copy data to GPU if needed\n",
        "        target = target.to(device)  # Copy target to GPU if needed\n",
        "\n",
        "        # Run an optimization step. See https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step\n",
        "        optimizer.zero_grad()  # Zero out gradients from previous step\n",
        "        output = model(data)  # Pass data through the network\n",
        "        loss = loss_fn(output, target)  # Calculate loss\n",
        "        train_loss += loss.data.item()  # Calculate loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "    train_loss /= len(dataloader_train)  # Calculate average loss\n",
        "    print(\n",
        "        \"Train Epoch: {} \\tLoss: {:.6f}\".format(\n",
        "            epoch,\n",
        "            train_loss,\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now train our model for 10 epochs and afterwards plot the training and validation losses over the training epochs. An epoch is a single pass through the training data.\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now plot the training and validation losses over the training epochs.\n",
        "\n",
        "Loss is a measure of how far a model's predictions are from its label, i.e. how bad the model is. The goal of training a model is to find a set of parameters that minimizes the loss. The loss is calculated using a loss function, which takes the model's prediction and the correct label as input and returns a number. The lower the loss, the better the model.\n",
        "\n",
        "The training process tries to find the set of parameters that minimizes the loss on the training data, i.e. the training loss. However, we also want to know how well the model performs on data it has never seen before, i.e. the validation data. If the model performs much worse on the validation data than the training data, it is memorizing patterns in the training data rather than learning general rules that apply to data in general. This is called overfitting."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We take note of the 20% dropout rate in the hidden layers. Dropout is a regularization technique that helps prevent overfitting. It works by randomly setting the outputs of neurons in a layer to zero during the forward pass of the training process. This has the effect of making the network more robust to changes in the weights of the network, and prevents the network from memorizing the training data. Using dropout also increases the loss on the training data, while explains why the training loss is higher than the validation loss."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Now that we have trained our model, we want to evaluate its performance on the test set. We do this to get an estimate of how well the model will perform on data it has never seen before. Technically, we should only do this once, at the very end of training, when we are completely finished with all hyperparameter tuning. However, at the end of this exercise, we will ask you to change some hyperparameters and retrain the model."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show model predictions and the true labels for a few images from the test dataset\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "test_dataset = datasets.MNIST(\"./data\", train=False, transform=transforms.ToTensor())\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "data, label = next(\n",
        "    iter(test_loader)\n",
        ")  # Get a batch of validation data (batch size = 32)\n",
        "data = data.to(device)  # Send data to device\n",
        "output = model(data)  # Pass data through the network\n",
        "pred = output.data.max(1)[\n",
        "    1\n",
        "]  # get the index of the max log-probability for each element in the batch\n",
        "\n",
        "# Graph 10 images and their predicted labels (green for correct, red for incorrect)\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i + 1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(data[i, :, :, :].cpu().numpy().reshape(28, 28), cmap=\"gray_r\")\n",
        "    plt.title(\n",
        "        f\"Pred: {pred[i].item()}\\nLabel: {label[i].item()}\",\n",
        "        color=(\"green\" if pred[i] == label[i] else \"red\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model tuning\n",
        "\n",
        "Modify the MLP model. Try to improve the classification accuracy, or experiment with the effects of different parameters.\n",
        "\n",
        "First try changing the learning rate `lr` for the optimizer and run the notebook again.\n",
        "\n",
        "Did you increase or decrease the learning rate? What was the effect on the training and validation losses? Try the opposite and see what happens.\n",
        "\n",
        "Next, try adding an additional hidden layer to the MLP model. How does this affect the training and validation losses?\n",
        "\n",
        "Finally, try changing the number of hidden units a the hidden layer. How does this affect the training and validation losses?\n",
        "\n",
        "You can also consult the PyTorch documentation at http://pytorch.org/."
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
